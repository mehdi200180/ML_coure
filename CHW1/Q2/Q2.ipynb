{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChC3RF8meAlK"
   },
   "source": [
    "<h1 align=\"center\">Introduction to Machine Learning - 25737-2</h1>\n",
    "<h4 align=\"center\">Dr. R. Amiri</h4>\n",
    "<h4 align=\"center\">Sharif University of Technology, Spring 2024</h4>\n",
    "\n",
    "\n",
    "**<font color='red'>Plagiarism is strongly prohibited!</font>**\n",
    "\n",
    "\n",
    "**Student Name**: Mehdi Abbaszadeh\n",
    "\n",
    "**Student ID**: 99106403\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IraiR0SbeDi_"
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRQjwWC3eDnc"
   },
   "source": [
    "**Task:** Implement your own Logistic Regression model, and test it on the given dataset of Logistic_question.csv!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "use_gpu = False \n",
    "\n",
    "if use_gpu:\n",
    "    import cupy as np \n",
    "else:\n",
    "    import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "class MyLogisticRegression:\n",
    "    # Your code goes here!\n",
    "    # This class must have an __init__ method, a loss function, a fit function, and a predict function. You also need to make your code runnable on gpu!\n",
    "    def __init__(self, learning_rate=0.01, regLambda = 0.01, num_iterations=100):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regLambda = regLambda\n",
    "        self.num_iterations = num_iterations\n",
    "        self.theta = None\n",
    "        self.loss_vector = np.array([])\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def loss_function(self, X, y):\n",
    "        m, n = X.shape\n",
    "        h = self.sigmoid(np.dot(X, self.theta))\n",
    "        regularization_term = self.regLambda/(2*m) * np.sum(np.square(self.theta[1:]))\n",
    "        loss = -1/m * (np.dot(y.T, np.log(h + 1e-10)) + np.dot((1 - y).T, np.log(1 - h + 1e-10))) + regularization_term\n",
    "        return loss.item()\n",
    "    \n",
    "    def gradient_descent(self, X, y):\n",
    "        m, n = X.shape\n",
    "        h = self.sigmoid(np.dot(X, self.theta))\n",
    "        regularization_term = self.regLambda/m * np.concatenate(([0], self.theta[1:])) \n",
    "        grad = 1/m * np.dot(X.T, (h - y)) + regularization_term\n",
    "        return grad\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        _, n = X.shape\n",
    "        self.theta = np.zeros(n, )\n",
    "        for _ in np.arange(self.num_iterations):\n",
    "            self.theta -= self.learning_rate*self.gradient_descent(X, y)\n",
    "            tmp_loss = self.loss_function(X, y)\n",
    "            self.loss_vector = np.append(self.loss_vector, tmp_loss)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        tmp = X.reshape(X.shape[0], len(self.theta))@self.theta.reshape(len(self.theta), 1)\n",
    "        y = self.sigmoid(tmp) > 0.5\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-i-oubUlZ6e"
   },
   "source": [
    "**Task:** Test your model on the given dataset. You must split your data into train and test, with a 0.2 split, then normalize your data using X_train data. Finally, report 4 different evaluation metrics of the model on the test set. (You might want to first make the Target column binary!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0KXzIy_2u-pG",
    "outputId": "9625f7e2-abb1-4591-c0fa-843525e0ffd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.925\n",
      "precision = 0.971830985915493\n",
      "recall = 0.9452054794520548\n",
      "f1 = 0.9583333333333334\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here!\n",
    "dataset = pd.read_csv('Q2/Logistic_question.csv')\n",
    "X = dataset.iloc[:, :-1].values\n",
    "Y = dataset.iloc[:, -1].values\n",
    "\n",
    "# add a column of 1's to the training data coressponding to Bias in weights vector\n",
    "\n",
    "# making the target column binary\n",
    "Y = Y > 0.5\n",
    "\n",
    "# train_test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state = 0)\n",
    "\n",
    "# normalize the data\n",
    "mean_tr = np.mean(X, axis=0)\n",
    "std_tr = np.std(X, axis=0)\n",
    "X_train = (X_train - mean_tr)/std_tr\n",
    "\n",
    "mean_test = np.mean(X_test, axis=0)\n",
    "std_test = np.std(X_test, axis=0)\n",
    "X_test = (X_test - mean_test)/std_test\n",
    "\n",
    "# instantiate the MyLogisticRegression class\n",
    "my_model = MyLogisticRegression()\n",
    "my_model.fit(X_train, y_train)\n",
    "# plt.plot(np.arange(1, len(my_model.loss_vector) + 1), my_model.loss_vector)\n",
    "# plt.show()\n",
    "y_pred = my_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f'accuracy = {accuracy}')\n",
    "print(f'precision = {precision}')\n",
    "print(f'recall = {recall}')\n",
    "print(f'f1 = {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ji0RXNGKv1pa"
   },
   "source": [
    "**Question:** What are each of your used evaluation metrics? And for each one, mention situations in which they convey more data on the model performance in specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldveD35twRRZ"
   },
   "source": [
    "**Your answer:**\n",
    "\n",
    "**1.Accuracy:**\n",
    "Accuracy measures the proportion of correctly classified instances out of the total number of instances.\n",
    "Accuracy is useful when the class distribution is balanced and the cost of misclassification for both classes is similar.However, accuracy can be misleading when the class distribution is imbalanced, as it may overestimate the performance of the model when one class dominates the dataset\n",
    "\n",
    "**2.Precision:**\n",
    "Precision measures the proportion of true positive predictions out of all positive predictions made by the model.\n",
    "Precision is useful when the cost of false positives (incorrectly predicting the positive class) is high. It emphasizes the model's ability to avoid false positives.\n",
    "For example, in spam email detection, precision is important because misclassifying non-spam emails as spam (false positives) can be disruptive to users.\n",
    "\n",
    "**3.Recall:**\n",
    "Recall measures the proportion of true positive predictions out of all actual positive instances in the dataset.\n",
    "Recall is useful when the cost of false negatives (incorrectly predicting the negative class) is high. It emphasizes the model's ability to capture all positive instances.\n",
    "For example, in medical diagnosis, recall is important because missing positive cases (false negatives) can be life-threatening if not detected.\n",
    "\n",
    "**4.F1 Score:**\n",
    "The F1 score is the harmonic mean of precision and recall, providing a balance between the two metrics.\n",
    "The F1 score is useful when there is an uneven class distribution or when both false positives and false negatives are costly.\n",
    "It is particularly valuable when you need to consider both precision and recall simultaneously and want a single metric to evaluate the model's performance comprehensively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ZCeRHZSw-mh"
   },
   "source": [
    "**Task:** Now test the built-in function of Python for Logistic Regression, and report all the same metrics used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Vb5lRSQXDLR3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.95\n",
      "precision = 0.96\n",
      "recall = 0.9863013698630136\n",
      "f1 = 0.9729729729729729\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here!\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f'accuracy = {accuracy}')\n",
    "print(f'precision = {precision}')\n",
    "print(f'recall = {recall}')\n",
    "print(f'f1 = {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCvIymmMy_ji"
   },
   "source": [
    "**Question:** Compare your function with the built-in function. On the matters of performance and parameters. Briefly explain what the parameters of the built-in function are and how they affect the model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EY0ohM16z3De"
   },
   "source": [
    "**Your answer:**\n",
    "as you see, my function has better performance based on the metric precision. but in other three metrics the performance of built-in function is better than my function.\n",
    "When comparing a self-implemented logistic regression model with the built-in logistic regression model from scikit-learn, there are several factors to consider including performance, ease of use, and flexibility\n",
    "**1.Performance:**\n",
    "Performance can be evaluated in terms of accuracy, training time, and memory usage.\n",
    "Scikit-learn's built-in logistic regression model is optimized for performance and efficiency, often outperforming self-implemented models, especially for large datasets.\n",
    "Self-implemented models may not be as optimized and may suffer from performance issues, particularly when dealing with large-scale datasets.\n",
    "\n",
    "**2.Parameters:**\n",
    "Scikit-learn's logistic regression implementation offers several parameters that can be tuned to optimize model performance:\n",
    "penalty: Specifies the norm used in the penalization (L1 or L2 regularization).\n",
    "C: Inverse of regularization strength; smaller values specify stronger regularization.\n",
    "solver: Algorithm to use for optimization (e.g., 'liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga').\n",
    "max_iter: Maximum number of iterations for optimization algorithms.\n",
    "class_weight: Weights associated with classes to address class imbalance.\n",
    "These parameters can significantly affect the model's performance and generalization ability:\n",
    "Choosing the right regularization strength (C) helps prevent overfitting.\n",
    "The choice of solver and maximum number of iterations can impact convergence speed and memory usage.\n",
    "Setting appropriate class weights is crucial for handling imbalanced datasets.\n",
    "\n",
    "**3.Ease of use and flexibility:**\n",
    "Scikit-learn's logistic regression implementation provides a user-friendly interface and seamlessly integrates with other scikit-learn functionalities (e.g., cross-validation, pipeline).\n",
    "Self-implemented models require more effort to develop, debug, and maintain. They may lack some advanced features available in scikit-learn, such as built-in cross-validation and hyperparameter tuning.\n",
    "However, self-implemented models offer greater flexibility and customization, allowing researchers to experiment with different optimization algorithms, regularization techniques, and custom loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClMqoYlr2kr7"
   },
   "source": [
    "# Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukvlqDe52xP5"
   },
   "source": [
    "**Task:** Implement your own Multinomial Logistic Regression model. Your model must be able to handle any number of labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5Ir-_hFt286t"
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class MyMultinomialLogisticRegression:\n",
    "    # Your code goes here!\n",
    "    # This class must have an __init__ method, a loss function, a fit function, and a predict function. You also need to make your code runnable on gpu!\n",
    "    def __init__(self, learning_rate=0.01, regLambda = 0.01, num_iterations=100):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regLambda = regLambda\n",
    "        self.num_iterations = num_iterations\n",
    "        self.regLambda = regLambda\n",
    "        self.theta = None\n",
    "        \n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "    \n",
    "    def loss_function(self, X, y):\n",
    "        m = len(y)\n",
    "        h = self.softmax(np.dot(X, self.theta))\n",
    "        cross_entropy_loss = -1/m * np.sum(np.log(h[np.arange(m), y]))\n",
    "        regularization_term = 0.5 * self.regLambda * np.sum(self.theta ** 2)/m\n",
    "        cost = cross_entropy_loss + regularization_term\n",
    "        return cost\n",
    "    \n",
    "    def gradient_descent(self, X, y):\n",
    "        m, n = X.shape\n",
    "        num_classes = len(np.unique(y))\n",
    "        self.theta = np.zeros((n, num_classes))\n",
    "        y_one_hot = np.eye(num_classes)[y]\n",
    "        for _ in range(self.num_iterations):\n",
    "            h = self.softmax(np.dot(X, self.theta))\n",
    "            gradient = 1/m * np.dot(X.T, (h - y_one_hot))\n",
    "            gradient += 1/m*self.regLambda * self.theta  \n",
    "            self.theta -= self.learning_rate * gradient\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        self.gradient_descent(X, y)\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        return self.softmax(np.dot(X, self.theta))\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predict class labels\n",
    "        return np.argmax(self.predict_proba(X), axis=1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPQ3Rtay3Y2_"
   },
   "source": [
    "**Task:** Test your model on the given dataset. Do the same as the previous part, but here you might want to first make the Target column quantized into $i$ levels. Change $i$ from 2 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9aP4QJPq29B3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for i = 2\n",
      "accuracy = 1.0\n",
      "precision = 1.0\n",
      "recall = 1.0\n",
      "f1 = 1.0\n",
      "*******************************\n",
      "results for i = 3\n",
      "accuracy = 0.8375\n",
      "precision = 0.8584300451321727\n",
      "recall = 0.8375\n",
      "f1 = 0.8415086423896627\n",
      "*******************************\n",
      "results for i = 4\n",
      "accuracy = 0.725\n",
      "precision = 0.6458802376485304\n",
      "recall = 0.725\n",
      "f1 = 0.6792413853727144\n",
      "*******************************\n",
      "results for i = 5\n",
      "accuracy = 0.55\n",
      "precision = 0.5803646318732525\n",
      "recall = 0.55\n",
      "f1 = 0.4961309523809523\n",
      "*******************************\n",
      "results for i = 6\n",
      "accuracy = 0.5375\n",
      "precision = 0.5328125\n",
      "recall = 0.5375\n",
      "f1 = 0.4760957792207792\n",
      "*******************************\n",
      "results for i = 7\n",
      "accuracy = 0.4125\n",
      "precision = 0.35320881226053635\n",
      "recall = 0.4125\n",
      "f1 = 0.3300271739130435\n",
      "*******************************\n",
      "results for i = 8\n",
      "accuracy = 0.3625\n",
      "precision = 0.42699362041467304\n",
      "recall = 0.3625\n",
      "f1 = 0.2709020301312391\n",
      "*******************************\n",
      "results for i = 9\n",
      "accuracy = 0.3\n",
      "precision = 0.2784137426900585\n",
      "recall = 0.3\n",
      "f1 = 0.24128415668326827\n",
      "*******************************\n",
      "results for i = 10\n",
      "accuracy = 0.2375\n",
      "precision = 0.17656733746130032\n",
      "recall = 0.2375\n",
      "f1 = 0.13719522267741507\n",
      "*******************************\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Your code goes here!\n",
    "dataset = pd.read_csv('Q2/Logistic_question.csv')\n",
    "X = dataset.iloc[:, :-1].values\n",
    "Y = dataset.iloc[:, -1].values\n",
    "\n",
    "levels = np.arange(2, 11)\n",
    "index = 0\n",
    "accuracy_vec = np.zeros((9, 1))\n",
    "\n",
    "for i in levels:\n",
    "    Y_new = np.digitize(Y, np.linspace(Y.min(), Y.max(), i)) - 1  # levels : [0, i - 1]\n",
    "    \n",
    "    # train_test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y_new, test_size=0.2, random_state = 0)\n",
    "\n",
    "    # normalize the data\n",
    "    mean_tr = np.mean(X, axis=0)\n",
    "    std_tr = np.std(X, axis=0)\n",
    "    X_train = (X_train - mean_tr)/std_tr\n",
    "\n",
    "    mean_test = np.mean(X_test, axis=0)\n",
    "    std_test = np.std(X_test, axis=0)\n",
    "    X_test = (X_test - mean_test)/std_test\n",
    "    \n",
    "    # instantiate the MyLogisticRegression class\n",
    "    my_model = MyMultinomialLogisticRegression()\n",
    "    my_model.fit(X_train, y_train)\n",
    "    y_pred = my_model.predict(X_test)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    accuracy_vec[index] = accuracy\n",
    "    index += 1\n",
    "\n",
    "    # printing the results\n",
    "    print(f'results for i = {i}')\n",
    "    print(f'accuracy = {accuracy}')\n",
    "    print(f'precision = {precision}')\n",
    "    print(f'recall = {recall}')\n",
    "    print(f'f1 = {f1}')\n",
    "    print('*******************************')\n",
    "    \n",
    "\n",
    "plt.plot(np.arange(1, index+1), accuracy_vec)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Of2sHl5Z4dXi"
   },
   "source": [
    "**Question:** Report for which $i$ your model performs best. Describe and analyze the results! You could use visualizations or any other method!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRLERDAr4wnS"
   },
   "source": [
    "**Your answer:**\n",
    "as you see the previous plot, the model performs best for i = 2. because ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wT43jGKV6CBZ"
   },
   "source": [
    "# Going a little further!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vo9uGo0R6GZo"
   },
   "source": [
    "First we download Adult income dataset from Kaggle! In order to do this create an account on this website, and create an API. A file named kaggle.json will be downloaded to your device. Then use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "o-vrjYBF7u1E",
    "outputId": "b274bc6e-4c35-4ad8-f17b-9e69f7d92923"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()  # Use this to select the kaggle.json file from your computer\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5i6u6_1v8ftX"
   },
   "source": [
    "Then use this code to automatically download the dataset into Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XjyVaVKF29Hx",
    "outputId": "15d0b1a2-c806-4102-abbc-12545237e218"
   },
   "outputs": [],
   "source": [
    "!kaggle datasets download -d wenruliu/adult-income-dataset\n",
    "!unzip /content/adult-income-dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXQnbZwt8rJK"
   },
   "source": [
    "**Task:** Determine the number of null entries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JtuEx6QW29c1",
    "outputId": "43397bec-0622-4dc4-de2b-c65be00e4503"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workclass: 2799 null entries\n",
      "occupation: 2809 null entries\n",
      "native-country: 857 null entries\n",
      "total number of null entries = 6465\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here!\n",
    "# reading the data\n",
    "adult = pd.read_csv('Q2/adult.csv')\n",
    "\n",
    "for i,j in zip(adult.columns,(adult.values.astype(str) == '?').sum(axis = 0)):\n",
    "    if j > 0:\n",
    "        print(str(i) + ': ' + str(j) + ' null entries')\n",
    "x = (adult.values.astype(str) == '?').sum(axis = 0).sum()\n",
    "print(f'total number of null entries = {x}')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpEcBdTUAYVN"
   },
   "source": [
    "**Question:** In many widely used datasets there are a lot of null entries. Propose 5 methods by which, one could deal with this problem. Briefly explain how do you decide which one to use in this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1u1pBHuAsSg"
   },
   "source": [
    "**Your answer:**\n",
    "\n",
    "There are different ways to handle missing values in Numeric data and Categorical data:\n",
    "\n",
    "**1. Mean/Median/Mode Imputation:**\n",
    "Replace null values with the mean, median, or mode of the column.\n",
    "This method is simple and effective when the null values are missing at random and donâ€™t have a significant impact on the overall distribution of the data.\n",
    "\n",
    "**2.Forward Fill:**\n",
    "The forwardfill() method is used to fill in missing values in a DataFrame or Series with the previous valid observation.\n",
    "\n",
    "**3. Backward Fill:**\n",
    "The backfill() method is used to fill in missing values in a DataFrame or Series with the next valid observation.\n",
    "\n",
    "**4. Interpolation:**\n",
    "Interpolation methods estimate missing values based on the values of neighboring data points. Common interpolation methods include linear interpolation and polynomial interpolation.\n",
    "Using Interpolate method for a column in a dataset.\n",
    "\n",
    "**8. Machine Learning Model:**\n",
    "You can use machine learning models to predict missing values based on the other features in your dataset.\n",
    "This can be a more sophisticated approach and might require splitting your data into two sets: one with non-null values for training and one with null values for prediction.\n",
    "\n",
    "**6. Replace with Constant Values:**\n",
    "Replace null values with a specific constant value that is meaningful for your dataset. For example, you might use 0 or -1 if it makes sense in your context.\n",
    "Replacing null values with a constant for a specific column.\n",
    "\n",
    "my method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHhH-hkpAxFf"
   },
   "source": [
    "**Task:** Handle null entries using your best method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "5fVwWcjK29fk",
    "outputId": "c21a6adf-1e6c-46d0-dd61-79d1710272c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['11th', 'HS-grad', 'Assoc-acdm', 'Some-college', '10th',\n",
       "       'Prof-school', '7th-8th', 'Bachelors', 'Masters', '5th-6th',\n",
       "       'Assoc-voc', '9th', 'Doctorate', '12th', '1st-4th', 'Preschool'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code goes here!\n",
    "# adult.replace('?', pd.NA, inplace=True)\n",
    "# adult.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43k5cTorCJaV"
   },
   "source": [
    "**Task:** Convert categorical features to numerical values. Split the dataset with 80-20 portion. Normalize all the data using X_train. Use the built-in Logistic Regression function and GridSearchCV to train your model, and report the parameters, train and test accuracy of the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Agj18Lcd-vyZ",
    "outputId": "69e132a9-0249-4a21-c8f3-45247c1e17dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'classifier__C': 0.01, 'classifier__solver': 'liblinear'}\n",
      "Train Accuracy:0.848246123227465\n",
      "Test Accuracy:0.83681592039801\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here!\n",
    "X = adult.iloc[:, :-1]\n",
    "Y = adult.iloc[:, -1]\n",
    "\n",
    "# data splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "# extract categorical_features\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "numeric_features = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# scaling function\n",
    "numeric_transformer = StandardScaler()\n",
    "# categorical to numeric function\n",
    "categorical_transformer = OneHotEncoder()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# pipeline for gridsearchcv function\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('classifier', LogisticRegression())])\n",
    "\n",
    "# Define parameters grid for grid search\n",
    "param_grid = {\n",
    "    'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
    "    'classifier__solver': ['liblinear', 'saga'],\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# fitting using gridsearch\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# driving the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# parameter of the best model\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# train and test accuracy of the best model\n",
    "train_predictions = best_model.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "test_predictions = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(f\"Train Accuracy:{train_accuracy}\")\n",
    "print(f\"Test Accuracy:{test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Lzr2lqXDQ1T"
   },
   "source": [
    "**Task:** To try a different route, split X_train into $i$ parts, and train $i$ separate models on these parts. Now propose and implement 3 different *ensemble methods* to derive the global models' prediction for X_test using the results(not necessarily predictions!) of the $i$ models. Firstly, set $i=10$ to find the method with the best test accuracy(the answer is not general!). You must Use your own Logistic Regression model.(You might want to modify it a little bit for this part!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "id": "K9D1jlstF9nF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['workclass', 'education', 'marital-status', 'occupation',\n",
      "       'relationship', 'race', 'gender', 'native-country'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here!\n",
    "print(categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QS9HYJ5FW1T"
   },
   "source": [
    "**Question:** Explain your proposed methods and the reason you decided to use them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hCBQuAeF46a"
   },
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjSREvg4FTHf"
   },
   "source": [
    "**Task:** Now, for your best method, change $i$ from 2 to 100 and report $i$, train and test accuracy of the best model. Also, plot test and train accuracy for $2\\leq i\\leq100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfKS-Jq0-v4P"
   },
   "outputs": [],
   "source": [
    "# Your code goes here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWV0YUgRGg1p"
   },
   "source": [
    "**Question:** Analyze the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
