# -*- coding: utf-8 -*-
"""Q2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1USoQ_6p9mjYFFkR7Z2qOjZbRI4jrg_Ur

<h1 align="center">Introduction to Machine Learning - Course Code: 25737</h1>
<h4 align="center">Instructor: Dr. Amiri</h4>
<h4 align="center">Sharif University of Technology, Spring 2024</h4>
<h4 align="center">Computer Assignment 3</h4>
<h4 align="center">

Question 2

</h4>

# Personal Data
"""

# Set your student number
student_number = 99106403
Name = 'Mehdi'
Last_Name = 'Abbaszadeh'

"""# Rules
- You are not allowed to add or remove cells. You **must use the provided space to write your code**. If you don't follow this rule, **your Practical Assignment won't be graded**.  

- Collaboration and using the internet is allowed, but your code **must be written by yourself**. **Copying code** from each other or from available resources will result in a **zero score for the assignment**.

- You are not allowed to use `torch.nn`, `torch.optim` and any activation function and loss function implemented in torch.

## Question 1) Convolutional Neural Networks ``(50 pt.)``

In this question, you'll be coding up a convolutional neural network from scratch to classify images using PyTorch.  

### Instructions
- Install PyTorch following the instructions [here](https://pytorch.org/).
- Install the [`torchinfo` package](https://github.com/TylerYep/torchinfo) to visualize the network architecture and the number of parameters. The maximum number of parameters you are allowed to use for your network is **100,000**.
- You are required to complete the functions defined in the code blocks following each question. Fill out sections of the code marked `"YOUR CODE HERE"`.
- You're free to add any number of methods within each class.
- You may also add any number of additional code blocks that you deem necessary.

### Data Preparation
In this assignment, we will use the Fashion-MNIST dataset. Fashion-MNIST is a dataset of Zalando's article imagesâ€”consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes.  

#### Data

Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255.  

#### Labels

Each training and test example is assigned to one of the following labels:

| Label | Description |
|-------|-------------|
| 0     | T-shirt/top |
| 1     | Trouser     |
| 2     | Pullover    |
| 3     | Dress       |
| 4     | Coat        |
| 5     | Sandal      |
| 6     | Shirt       |
| 7     | Sneaker     |
| 8     | Bag         |
| 9     | Ankle boot  |

Fashion-MNIST is included in the `torchvision` library.
"""

!pip install torchinfo

# Import required packages
from torchvision.datasets import FashionMNIST
from torchvision.transforms import Compose, ToTensor, Normalize
import matplotlib.pyplot as plt
import numpy as np

# Transform to normalize the data and convert to a tensor
transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])

# Download the data
dataset = FashionMNIST('MNIST_data/', download = True, train = True, transform = transform)

"""### Data Exploration
Let's take a look at the classes in our dataset.
"""

print(dataset.classes)

"""Now, visualize an instance from each class."""

######################
#   YOUR CODE HERE   #
classes = dataset.classes

# Create a dictionary to store one random image from each class
class_images = {class_name: None for class_name in classes}

# Plot one random image from each class
fig, axes = plt.subplots(2, 5, figsize=(15, 6))
axes = axes.flatten()

# Iterate over the dataset and randomly select one image from each class
for data, target in dataset:
    if class_images[classes[target]] is None:
        class_images[classes[target]] = data
        # Break the loop if all None targets have been replaced
        if None not in class_images.values():
            break

for i, (class_name, image) in enumerate(class_images.items()):
    axes[i].imshow(image.squeeze().numpy(), cmap='gray')
    axes[i].set_title(class_name)
    axes[i].axis('off')

plt.tight_layout()
plt.show()
######################

"""### Creating Training and Validation Datasets

The `split_indices` function takes in the size of the entire dataset, `n`, the fraction of data to be used as validation set, `val_frac`, and the random seed and returns the indices of the data points to be added to the validation dataset.
"""

def split_indices(n, val_frac, seed):
    # Determine the size of the validation set
    n_val = int(val_frac * n)
    np.random.seed(seed)
    # Create random permutation between 0 to n-1
    idxs = np.random.permutation(n)
    # Pick first n_val indices for validation set
    return idxs[n_val:], idxs[:n_val]

######################
#   YOUR CODE HERE   #
######################
val_frac =  0.15 ## Set the fraction for the validation set
rand_seed =  12 ## Set the random seed

train_indices, val_indices = split_indices(len(dataset), val_frac, rand_seed)
print("number of samples in training set: {}".format(len(train_indices)))
print("number of samples in validation set: {}".format(len(val_indices)))

"""Next, we make use of the built-in dataloaders in PyTorch to create iterables of our our training and validation sets. This helps in avoiding fitting the whole dataset into memory and only loads a batch of the data that we can decide."""

# Import required packages
from torch.utils.data.sampler import SubsetRandomSampler
from torch.utils.data.dataloader import DataLoader

######################
#   YOUR CODE HERE   #
######################
batch_size = 64 ## Set the batch size

# Training sampler and data loader
train_sampler = SubsetRandomSampler(train_indices)
train_dl = DataLoader(dataset,
                     batch_size,
                     sampler=train_sampler)

# Validation sampler and data loader
val_sampler = SubsetRandomSampler(val_indices)
val_dl = DataLoader(dataset,
                   batch_size,
                   sampler=val_sampler)

"""Plot images in a sample batch of data."""

######################
#   YOUR CODE HERE   #
# Plot one random image from each class
fig, axes = plt.subplots(8, 8, figsize=(15, 6))
axes = axes.flatten()

# Create an iterator for the training data loader
train_iter = iter(train_dl)

# Get the first batch
first_batch = next(train_iter)
inputs, targets = first_batch
# real target names
classes = dataset.classes
target_names = [classes[target] for target in targets]

for i, (image, class_name) in enumerate(zip(inputs, target_names)):
    axes[i].imshow(image.squeeze().numpy(), cmap='gray')
    axes[i].set_title(class_name)
    axes[i].axis('off')

plt.tight_layout()
plt.show()
######################

"""### Building the Model
Create your model by defining the network architecture in the `ImageClassifierNet` class.<br>
**NOTE:** The number of parameters in your network must be $\leq$ 100,000.
"""

# Import required packages
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchinfo import summary

class ImageClassifierNet(nn.Module):
    def __init__(self, n_channels=3):
        super(ImageClassifierNet, self).__init__()
        ######################
        #   YOUR CODE HERE   #
        ######################
        self.conv1 = nn.Sequential(
          nn.Conv2d(n_channels, 10, 3, 1, padding=1),    # (10, 28, 28)
          nn.ReLU(),
          nn.MaxPool2d(2, 2)                    # (10, 14, 14)
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(10, 20, 3, 1, padding=1), # (20, 14, 14)
            nn.ReLU(),
            nn.MaxPool2d(2, 2)                  # (20, 7, 7)
        )
        self.conv3 = nn.Sequential(
            nn.Conv2d(20, 40, 3, 1, padding=1), # (40, 7, 7)
            nn.ReLU(),
            nn.MaxPool2d(2, 2)                  # (40, 3, 3)
        )
        self.linear1 = nn.Sequential(
            nn.Linear(40 * 3 * 3, 100),
            nn.Dropout(0.2),
            nn.ReLU()
        )
        self.linear2 = nn.Linear(100, 10)

    def forward(self, X):
        ######################
        #   YOUR CODE HERE   #
        ######################
        X = self.conv1(X)
        X = self.conv2(X)
        X = self.conv3(X)
        X = X.view(-1, 40 * 3 * 3)
        X = self.linear1(X)
        X = F.log_softmax(self.linear2(X), dim=1)
        return X

model = ImageClassifierNet(n_channels = 1)

"""The following code block prints your network architecture. It also shows the total number of parameters in your network (see `Total params`).  

**NOTE: The total number of parameters in your model should be <= 100,000.**
"""

summary(model, input_size=(batch_size, 1, 28, 28))

"""### Train the model
Complete the `train_model` function to train your model on a dataset. Tune your network architecture and hyperparameters on the validation set.
"""

def train_model(n_epochs, model, train_dl, val_dl, loss_fn, opt_fn, lr):
    """
    Trains the model on a dataset.

    Args:
        n_epochs: number of epochs
        model: ImageClassifierNet object
        train_dl: training dataloader
        val_dl: validation dataloader
        loss_fn: the loss function
        opt_fn: the optimizer
        lr: learning rate

    Returns:
        The trained model.
        A tuple of (model, train_losses, val_losses, train_accuracies, val_accuracies)
    """
    # Record these values the end of each epoch
    train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []

    ######################
    #   YOUR CODE HERE   #
    ######################
    # setting the device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    best_val_loss = float('inf')
    optimizer = opt_fn(model.parameters(), lr=lr)  # Create optimizer


    for epoch in range(n_epochs):
        model.train()  # Set the model to training mode
        sum_loss, correct, total = 0, 0, 0
        for i, (inputs, labels) in enumerate(train_dl):
            inputs, labels = inputs.to(device), labels.to(device)
            pred_prob = model(inputs)
            _, y_pred = torch.max(pred_prob, 1)
            optimizer.zero_grad()
            loss = loss_fn(pred_prob, labels)
            # Backpropagation
            loss.backward()
            optimizer.step()
            # Calculate accuracy and losses
            correct += (y_pred == labels).float().sum()
            sum_loss += loss.item()*labels.shape[0]
            # computing total number of samples
            total += labels.shape[0]
        # evaluation the model after each batch training if required
        if len(val_dl) > 0:
            val_loss, val_acc = evaluate(device, model, val_dl)
            val_losses.append(val_loss)
            val_accuracies.append(val_acc)

        train_loss = sum_loss / total
        train_acc = correct / total

        train_losses.append(train_loss)
        train_accuracies.append(train_acc)

        if len(val_dl) > 0:
            print("EPOCH {:d}: train loss: {:.3f}, train_acc: {:.3f}, val loss: {:.3f}, val acc: {:.3f}"
            .format(epoch+1, train_loss, train_acc, val_loss, val_acc))
        else:
            print("EPOCH {:d}: train loss: {:.3f}, train_acc: {:.3f}"
            .format(epoch+1, train_loss, train_acc))


    return model, train_losses, val_losses, train_accuracies, val_accuracies


def evaluate(device, model, val_dl):
    model.eval()  # Set the model to evaluation model
    correct, total, sum_loss = 0, 0, 0
    for i, (inputs, labels) in enumerate(val_dl):
        inputs, labels = inputs.to(device), labels.to(device)
        pred_prob = model(inputs)
        loss = loss_fn(pred_prob, labels)
        _, y_pred = torch.max(pred_prob, 1)
        sum_loss += loss.item()*labels.shape[0]
        correct += (y_pred == labels).float().sum()
        total += labels.shape[0]
    return sum_loss / total, correct / total

######################
#   YOUR CODE HERE   #
######################
num_epochs = 25 # Number of training epochs
loss_fn =  F.nll_loss # Define the loss function
opt_fn =  torch.optim.Adam # Select an optimizer
lr =  0.001 # Set the learning rate

model, train_losses, val_losses, train_accuracies, val_accuracies = train_model(num_epochs, model, train_dl, val_dl, loss_fn, opt_fn, lr)

"""### Plot loss and accuracy"""

def plot_accuracy(train_accuracies, val_accuracies):
    """Plot accuracies"""
    plt.plot(train_accuracies, "-x")
    plt.plot(val_accuracies, "-o")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.legend(["Training", "Validation"])
    plt.title("Accuracy vs. No. of epochs")

def plot_losses(train_losses, val_losses):
    """Plot losses"""
    plt.plot(train_losses, "-x")
    plt.plot(val_losses, "-o")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend(["Training", "Validation"])
    plt.title("Loss vs. No. of Epochs")

# plot the losses
plot_losses(train_losses, val_losses)

# plot the accuracy
plot_accuracy(train_accuracies, val_accuracies)

"""### Train a model on the entire dataset"""

indices, _ = split_indices(len(dataset), 0, rand_seed)

sampler = SubsetRandomSampler(indices)
dl = DataLoader(dataset, batch_size, sampler=sampler)

######################
#   YOUR CODE HERE   #
######################
num_epochs = 25# Max number of training epochs
lr = 0.001# Set the learning rate

model, _, _, _, _ = train_model(num_epochs, model, dl, [], loss_fn, opt_fn, lr)

"""### Check Predictions
Now, you should evaluate your model on dataset. Specifically, you should calculate the class probabilities for each image, and then visualize these probabilities along with the image itself. Please include a function `view_prediction` in your code that takes an image, its label, the calculated probabilities, and the list of class names as input, and plots the image along with the probabilities.
"""

######################
#   YOUR CODE HERE   #
######################
def view_prediction(image, label, prob_vector, class_names):
    fig, axes = plt.subplots(2, 1,figsize=(8, 8))
    axes = axes.flatten()
    # image
    axes[0].imshow(image.squeeze().numpy(), cmap='gray')
    axes[0].set_title(f'True Label: {class_names[label]}')
    axes[0].axis('off')
    # probability
    y_pos = np.arange(len(class_names))
    axes[1].barh(y_pos, prob_vector)
    axes[1].yticks(y_pos, class_names)
    axes[1].xlabel('Probability')
    axes[1].title('Class Probabilities')
    axes[1].xlim(0, 1)

    plt.tight_layout()
    plt.show()

"""### Save the model"""

torch.save(model, 'model')

"""### Calculate accuracy on the test set"""

test_dataset = FashionMNIST('MNIST_data/', download = True, train = False, transform = transform)
test_dl = DataLoader(test_dataset, batch_size)

def evaluate(model, test_dl):
    """
    Evaluates your model on the test data.

    Args:
        model: ImageClassifierNet object
        test_dl: test dataloader

    Returns:
        Test accuracy.
    """
    ######################
    #   YOUR CODE HERE   #
    ######################
    model.eval()  # Set the model to evaluation model
    correct, total = 0, 0
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    for i, (inputs, labels) in enumerate(test_dl):
        inputs, labels = inputs.to(device), labels.to(device)
        pred_prob = model(inputs)
        _, y_pred = torch.max(pred_prob, 1)
        correct += (y_pred == labels).float().sum()
        total += labels.shape[0]
        # print('batch : {:d}, accuracy : {: .3f}, batch_size : {:d}'.format(i+1, (y_pred == labels).float().sum()/labels.shape[0], labels.shape[0]))

    return correct / total

print("Test Accuracy = {:.4f}".format(evaluate(model, test_dl)))